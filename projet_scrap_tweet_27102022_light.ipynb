{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d63d435",
   "metadata": {},
   "source": [
    "Ce notebook présente différentes algorithmes de Machine Learning appliqués à des tweets.\n",
    "Il s'agit d'un projet d'étude réalisé dans le cadre du DU Data Analysis de la Sorbonne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5c9e6-5e31-47fc-8a07-996f4fe08c2c",
   "metadata": {},
   "source": [
    "# 1. Import des données\n",
    "* https://mihaelagrigore.medium.com/scraping-historical-tweets-without-a-twitter-developer-account-79a2c61f76ab\n",
    "* https://github.com/JustAnotherArchivist/snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f63526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yellowbrick\n",
      "  Downloading yellowbrick-1.5-py3-none-any.whl (282 kB)\n",
      "     -------------------------------------- 282.6/282.6 kB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.7.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (3.5.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.0.2)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Installing collected packages: yellowbrick\n",
      "Successfully installed yellowbrick-1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1889ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 15.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.21.5)\n",
      "Requirement already satisfied: numexpr in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: future in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sylvi\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis) (65.3.0)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.4)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (pyproject.toml): started\n",
      "  Building wheel for pyLDAvis (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136882 sha256=9e43054590da8d3a285a2de0520369af1f1dfb74fb95af320bbd7033e7de3110\n",
      "  Stored in directory: c:\\users\\sylvi\\appdata\\local\\pip\\cache\\wheels\\57\\a4\\86\\d10c6c2e0bf149fbc0afb0aa5a6528ac35b30a133a0270c477\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=28447f0db4f9f157d1fa51e7cbf4c81634c46de822a2c87046db843363e24d76\n",
      "  Stored in directory: c:\\users\\sylvi\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: funcy, sklearn, pyLDAvis\n",
      "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3275c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simplemma\n",
      "  Downloading simplemma-0.9.0-py3-none-any.whl (76.2 MB)\n",
      "     --------------------------------------- 76.2/76.2 MB 32.8 MB/s eta 0:00:00\n",
      "Installing collected packages: simplemma\n",
      "Successfully installed simplemma-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install simplemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63efff1b-6e28-4376-afaa-cfb1d4d9d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sylvi\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:400: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\Users\\sylvi\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\sylvi\\anaconda3\\lib\\site-packages\\yellowbrick\\style\\colors.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  mpl_ge_150 = LooseVersion(mpl.__version__) >= \"1.5.0\"\n",
      "C:\\Users\\sylvi\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\sylvi\\anaconda3\\lib\\site-packages\\yellowbrick\\style\\rcmod.py:31: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  mpl_ge_150 = LooseVersion(mpl.__version__) >= \"1.5.0\"\n",
      "C:\\Users\\sylvi\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import uuid\n",
    "\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour le pré processing\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Pour la dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Les bigrammes\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Pour la vectorisation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Pour la modélisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "import emojis\n",
    "\n",
    "sns.set()\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "import simplemma\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "\n",
    "from nltk.cluster import KMeansClusterer\n",
    "\n",
    "\n",
    "pd.set_option('display.min_rows', 50)\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # setting ignore as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6deac8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choix ancien fichier=0, Choix nouvelle recherche=1 : 0\n"
     ]
    }
   ],
   "source": [
    "# Choix entre un fichier existant ou une nouvelle recherche\n",
    "choix_data=int(input('Choix ancien fichier=0, Choix nouvelle recherche=1 : '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc1f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if choix_data==1:\n",
    "\n",
    "    # Scrap d'un nouveau fichier et Sauvegarde sous format json\n",
    "    json_filename = 'Ukraine-query-tweets-light.json'\n",
    "\n",
    "    #Using the OS library to call CLI commands in Python\n",
    "    os.system(f'snscrape --max-results 50000 --jsonl --progress --since 2022-02-24 twitter-search \"#Ukraine lang:fr until:2022-08-18\" > {json_filename}')\n",
    "\n",
    "elif choix_data==0:\n",
    "\n",
    "    # Utilisation d'un fichier json existant\n",
    "    json_filename = 'Ukraine-query-tweets-v2.json'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb5057e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>inReplyToTweetId</th>\n",
       "      <th>inReplyToUser</th>\n",
       "      <th>mentionedUsers</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/M_Degage/status/1560052163212152837</td>\n",
       "      <td>2022-08-17 23:53:14+00:00</td>\n",
       "      <td>🔝#INFO à #RT 🙏💖 \\n🇫🇷 #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\n🚨Jour_175 #GUERRE #Ukraine + #OTAN &amp;gt;&amp;gt; #RUSSIE + #...</td>\n",
       "      <td>🔝#INFO à #RT 🙏💖 \\n🇫🇷 #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\n🚨Jour_175 #GUERRE #Ukraine + #OTAN &amp;gt;&amp;gt; #RUSSIE + #...</td>\n",
       "      <td>1560052163212152837</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': '🇫🇷FIL sous MACRO. Dictateur-qui-ment...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Photo', 'previewUrl': 'https://pbs.twimg.com/media/FaZidccWQAAiiDX?format=jpg&amp;name=small', 'fullUrl': 'https:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': '🇫🇷FIL sous MACRO. Dictateur-qui-men...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[INFO, RT, FR, RU, eZ, GJ, JB, lr, Zemmour, Zozz, Patriotes, JambonBeurre, GUERRE, Ukraine, OTAN, RUSSIE, Donbass, DernièresInfos]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/millimagino/status/1560051755286757377</td>\n",
       "      <td>2022-08-17 23:51:37+00:00</td>\n",
       "      <td>#Ukraine / Centrale nucléaire de Zaporijjia: Kiev affirme qu'il faut se \"préparer à tous les scénarios\" https://t.co/PA2qLQUUEb via @BFMTV</td>\n",
       "      <td>#Ukraine / Centrale nucléaire de Zaporijjia: Kiev affirme qu'il faut se \"préparer à tous les scénarios\" bfmtv.com/international/… via @BFMTV</td>\n",
       "      <td>1560051755286757377</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'millimagino', 'id': 196342852, 'displayname': '𝓢𝓮𝓻𝓰𝓮 (Gros Loup) ♪', 'description': '🎸 ◮ ✪...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': 'BFMTV', 'id': 133663801, 'displayname': 'BFMTV', 'description': None, 'rawDescription': N...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/Lejojo66/status/1560049701315018752</td>\n",
       "      <td>2022-08-17 23:43:27+00:00</td>\n",
       "      <td>@WAW_AgainstWar Je me demande ce qu’ils vont dire aux peuples russe ses présentateurs a la co…quand la Poutine va perdre Kherson et la Crimée. En ...</td>\n",
       "      <td>@WAW_AgainstWar Je me demande ce qu’ils vont dire aux peuples russe ses présentateurs a la co…quand la Poutine va perdre Kherson et la Crimée. En ...</td>\n",
       "      <td>1560049701315018752</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'Lejojo66', 'id': 1432071090545831944, 'displayname': 'Jojo', 'description': '', 'rawDescri...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.559957e+18</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': '₩AW: War Against War', 'descri...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': '₩AW: War Against War', 'descr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/UkrinformFra/status/1560048041670975488</td>\n",
       "      <td>2022-08-17 23:36:52+00:00</td>\n",
       "      <td>Guerre en Ukraine : Deux civils tués et sept blessés dans la région de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...</td>\n",
       "      <td>Guerre en Ukraine : Deux civils tués et sept blessés dans la région de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...</td>\n",
       "      <td>1560048041670975488</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'UkrinformFra', 'id': 835057652937916416, 'displayname': 'Ukrinform en français', 'descript...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine, UkraineRussie, UkraineInvasion, Russie, GuerreEnUkraine, guerreUkraine, Donetsk]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/ErikBoesch/status/1560047415612379136</td>\n",
       "      <td>2022-08-17 23:34:22+00:00</td>\n",
       "      <td>@24hPujadas @trinquand Berlin a-t-il connaissance de la présence en #Ukraine de matériel militaire qui pourrait avoir été remis à #Kiev par des Br...</td>\n",
       "      <td>@24hPujadas @trinquand Berlin a-t-il connaissance de la présence en #Ukraine de matériel militaire qui pourrait avoir été remis à #Kiev par des Br...</td>\n",
       "      <td>1560047415612379136</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'ErikBoesch', 'id': 1946616553, 'displayname': 'Erik P. Alfred', 'description': 'Vive l'ami...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.559937e+18</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None, ...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine, Kiev]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _type  \\\n",
       "0  snscrape.modules.twitter.Tweet   \n",
       "1  snscrape.modules.twitter.Tweet   \n",
       "2  snscrape.modules.twitter.Tweet   \n",
       "3  snscrape.modules.twitter.Tweet   \n",
       "4  snscrape.modules.twitter.Tweet   \n",
       "\n",
       "                                                           url  \\\n",
       "0      https://twitter.com/M_Degage/status/1560052163212152837   \n",
       "1   https://twitter.com/millimagino/status/1560051755286757377   \n",
       "2      https://twitter.com/Lejojo66/status/1560049701315018752   \n",
       "3  https://twitter.com/UkrinformFra/status/1560048041670975488   \n",
       "4    https://twitter.com/ErikBoesch/status/1560047415612379136   \n",
       "\n",
       "                       date  \\\n",
       "0 2022-08-17 23:53:14+00:00   \n",
       "1 2022-08-17 23:51:37+00:00   \n",
       "2 2022-08-17 23:43:27+00:00   \n",
       "3 2022-08-17 23:36:52+00:00   \n",
       "4 2022-08-17 23:34:22+00:00   \n",
       "\n",
       "                                                                                                                                                 content  \\\n",
       "0  🔝#INFO à #RT 🙏💖 \\n🇫🇷 #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\n🚨Jour_175 #GUERRE #Ukraine + #OTAN &gt;&gt; #RUSSIE + #...   \n",
       "1             #Ukraine / Centrale nucléaire de Zaporijjia: Kiev affirme qu'il faut se \"préparer à tous les scénarios\" https://t.co/PA2qLQUUEb via @BFMTV   \n",
       "2  @WAW_AgainstWar Je me demande ce qu’ils vont dire aux peuples russe ses présentateurs a la co…quand la Poutine va perdre Kherson et la Crimée. En ...   \n",
       "3  Guerre en Ukraine : Deux civils tués et sept blessés dans la région de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...   \n",
       "4  @24hPujadas @trinquand Berlin a-t-il connaissance de la présence en #Ukraine de matériel militaire qui pourrait avoir été remis à #Kiev par des Br...   \n",
       "\n",
       "                                                                                                                                         renderedContent  \\\n",
       "0  🔝#INFO à #RT 🙏💖 \\n🇫🇷 #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\n🚨Jour_175 #GUERRE #Ukraine + #OTAN &gt;&gt; #RUSSIE + #...   \n",
       "1           #Ukraine / Centrale nucléaire de Zaporijjia: Kiev affirme qu'il faut se \"préparer à tous les scénarios\" bfmtv.com/international/… via @BFMTV   \n",
       "2  @WAW_AgainstWar Je me demande ce qu’ils vont dire aux peuples russe ses présentateurs a la co…quand la Poutine va perdre Kherson et la Crimée. En ...   \n",
       "3  Guerre en Ukraine : Deux civils tués et sept blessés dans la région de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...   \n",
       "4  @24hPujadas @trinquand Berlin a-t-il connaissance de la présence en #Ukraine de matériel militaire qui pourrait avoir été remis à #Kiev par des Br...   \n",
       "\n",
       "                    id  \\\n",
       "0  1560052163212152837   \n",
       "1  1560051755286757377   \n",
       "2  1560049701315018752   \n",
       "3  1560048041670975488   \n",
       "4  1560047415612379136   \n",
       "\n",
       "                                                                                                                                                    user  \\\n",
       "0  {'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': '🇫🇷FIL sous MACRO. Dictateur-qui-ment...   \n",
       "1  {'_type': 'snscrape.modules.twitter.User', 'username': 'millimagino', 'id': 196342852, 'displayname': '𝓢𝓮𝓻𝓰𝓮 (Gros Loup) ♪', 'description': '🎸 ◮ ✪...   \n",
       "2  {'_type': 'snscrape.modules.twitter.User', 'username': 'Lejojo66', 'id': 1432071090545831944, 'displayname': 'Jojo', 'description': '', 'rawDescri...   \n",
       "3  {'_type': 'snscrape.modules.twitter.User', 'username': 'UkrinformFra', 'id': 835057652937916416, 'displayname': 'Ukrinform en français', 'descript...   \n",
       "4  {'_type': 'snscrape.modules.twitter.User', 'username': 'ErikBoesch', 'id': 1946616553, 'displayname': 'Erik P. Alfred', 'description': 'Vive l'ami...   \n",
       "\n",
       "   replyCount  retweetCount  likeCount  ...  \\\n",
       "0           0             6          8  ...   \n",
       "1           0             0          0  ...   \n",
       "2           1             0          0  ...   \n",
       "3           0             0          0  ...   \n",
       "4           0             0          0  ...   \n",
       "\n",
       "                                                                                                                                                   media  \\\n",
       "0  [{'_type': 'snscrape.modules.twitter.Photo', 'previewUrl': 'https://pbs.twimg.com/media/FaZidccWQAAiiDX?format=jpg&name=small', 'fullUrl': 'https:...   \n",
       "1                                                                                                                                                   None   \n",
       "2                                                                                                                                                   None   \n",
       "3                                                                                                                                                   None   \n",
       "4                                                                                                                                                   None   \n",
       "\n",
       "   retweetedTweet quotedTweet inReplyToTweetId  \\\n",
       "0             NaN        None              NaN   \n",
       "1             NaN        None              NaN   \n",
       "2             NaN        None     1.559957e+18   \n",
       "3             NaN        None              NaN   \n",
       "4             NaN        None     1.559937e+18   \n",
       "\n",
       "                                                                                                                                           inReplyToUser  \\\n",
       "0                                                                                                                                                   None   \n",
       "1                                                                                                                                                   None   \n",
       "2  {'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': '₩AW: War Against War', 'descri...   \n",
       "3                                                                                                                                                   None   \n",
       "4  {'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None, ...   \n",
       "\n",
       "                                                                                                                                          mentionedUsers  \\\n",
       "0  [{'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': '🇫🇷FIL sous MACRO. Dictateur-qui-men...   \n",
       "1  [{'_type': 'snscrape.modules.twitter.User', 'username': 'BFMTV', 'id': 133663801, 'displayname': 'BFMTV', 'description': None, 'rawDescription': N...   \n",
       "2  [{'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': '₩AW: War Against War', 'descr...   \n",
       "3                                                                                                                                                   None   \n",
       "4  [{'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None,...   \n",
       "\n",
       "  coordinates place  \\\n",
       "0        None  None   \n",
       "1        None  None   \n",
       "2        None  None   \n",
       "3        None  None   \n",
       "4        None  None   \n",
       "\n",
       "                                                                                                                             hashtags  \\\n",
       "0  [INFO, RT, FR, RU, eZ, GJ, JB, lr, Zemmour, Zozz, Patriotes, JambonBeurre, GUERRE, Ukraine, OTAN, RUSSIE, Donbass, DernièresInfos]   \n",
       "1                                                                                                                           [Ukraine]   \n",
       "2                                                                                                                           [Ukraine]   \n",
       "3                                          [Ukraine, UkraineRussie, UkraineInvasion, Russie, GuerreEnUkraine, guerreUkraine, Donetsk]   \n",
       "4                                                                                                                     [Ukraine, Kiev]   \n",
       "\n",
       "   cashtags  \n",
       "0      None  \n",
       "1      None  \n",
       "2      None  \n",
       "3      None  \n",
       "4      None  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Création d'un dataframe df contentant les tweets\n",
    "\n",
    "df = pd.read_json(json_filename, lines=True)\n",
    "\n",
    "# Sauvegarde d'une copie\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Visualisation de df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "783b419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scrap_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa987a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'projet_scrap_tweet_27102022_light.ipynb',\n",
       " 'scarp_tweet_ukraine_29092022.ipynb',\n",
       " 'scrap_tweets.csv',\n",
       " 'scrap_tweet_ukraine_01102022.ipynb',\n",
       " 'scrap_tweet_ukraine_20092022.ipynb',\n",
       " 'Ukraine-query-tweets-light.json',\n",
       " 'Ukraine-query-tweets-v2.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d26f8",
   "metadata": {},
   "source": [
    "# 2. Premières analyses: évolution du nombre de tweets dans le temps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700a647",
   "metadata": {},
   "source": [
    "## 2.1 Evolution du nombre de tweets dans le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c18fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de df2: 2 colonnes= day + count=nbre de tweets par jour\n",
    "df['day'] = df['date'].dt.strftime('%D') \n",
    "df['week_number'] = df['date'].dt.strftime('%V') \n",
    "df2=df.groupby(['day']).size().reset_index(name='counts')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution du nombre de tweets par jour\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "x = df2['day']\n",
    "y = df2['counts']\n",
    "plt.plot(x, y)\n",
    "_ = ax.set_xticks(x[::5])\n",
    "_ = ax.set_xticklabels(x[::5], rotation=45)\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "#_ = plt.xlabel('Jour')\n",
    "_ = plt.ylabel('Nombre de tweets par jour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de df3: 2 colonnes= semaine + count=nbre de tweets par semaine\n",
    "df3=df.groupby(['week_number']).size().reset_index(name='counts')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution du nombre de tweets par semaine\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(df3['week_number'], df3['counts'])\n",
    "_ = ax.set_xlabel('Week Number')\n",
    "_ = ax.set_ylabel('Number of tweets')\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b0ac6",
   "metadata": {},
   "source": [
    "## 2.2 Nombre de mots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths = np.array(list(map(len, df['content'].str.split(' '))))\n",
    "\n",
    "print(f\"Le nombre moyen de mots par tweet est : {int(np.mean(document_lengths))}.\")\n",
    "\n",
    "\n",
    "print(f\"Le nombre minimum de mots par tweet est: {min(document_lengths)}.\")\n",
    "print(f\"Le nombre maximum de mots par tweet est: {max(document_lengths)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "_=ax.set_title(\"Distribution of number of words before preprocessing\", fontsize=16)\n",
    "_=ax.set_xlabel(\"Number of words\")\n",
    "_=sns.distplot(document_lengths, bins=50, ax=ax)\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b56ff",
   "metadata": {},
   "source": [
    "## 2.3. Analyse des emoji pour préparer une analyse de sentiments (non traitée ici)\n",
    "* https://emojis.readthedocs.io/en/latest/api.html#module-emojis\n",
    "* https://www.kaggle.com/code/infamouscoder/emoji-sentiment-features\n",
    "* https://www.kaggle.com/code/infamouscoder/emoji-sentiment-features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a93fdc",
   "metadata": {},
   "source": [
    "### 2.3.1. Analyse globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab800ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_emoji(my_pd):\n",
    "    \"\"\"\n",
    "    Crée une liste des labels des emojis en francais.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "\n",
    "    my_list : liste d'emojis.\n",
    "\n",
    "    \"\"\"\n",
    "    col_emo=[]\n",
    "    my_set=emojis.get(my_pd)\n",
    "    #for emo in my_set:\n",
    "    #    val = emoji.demojize(emo, language='fr').split(':')[1]\n",
    "    #    col_emo.append(val)\n",
    "    return list(my_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne qui contient tous les émojis des tweets\n",
    "df['emoji']=df['content'].apply((lambda x : create_column_emoji(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83df78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le % de tweets comportant des emojis est: {np.ceil(100*df.loc[(df['emoji'].str.len() != 0),:].shape[0]/df.shape[0])}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emoji'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_emoji = df[df['emoji'].str.len() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091ab16",
   "metadata": {},
   "source": [
    "Cela n'est pas suffisant pour une prise en compte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229d3b9",
   "metadata": {},
   "source": [
    "### 2.3.2. Preprocessing pour une analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd16861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste de tous les emojis presents\n",
    "#res_list = [y for x in df['emoji'] for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89487ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste d'emojis uniques\n",
    "#emo_list = list(set(res_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa17a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(emo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emoji = df[df['emoji'].str.len() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de nouvelles colonnes correspondant aux sentiments exprimés\n",
    "\n",
    "df_emoji['positive_emoji'] = 0\n",
    "df_emoji['neutral_emoji'] = 0\n",
    "df_emoji['negative_emoji'] = 0\n",
    "\n",
    "positive_emoji = ['❤️','❤','😍','♥️','😊','💕','👍','😂','🙌','🤑','💖','✨','😊','🎉','💞','😝','😈','😃','😁','😎','😘','💓','😉','😬','😄','😀','😜','💗','😌','😆','😛','😻','🙋','❣️','🙂','😇','💝','😏','😋','🤗','🙆','🤓','😚','😙','😸','😼','😺','😽']\n",
    "neutral_emoji = ['🙏','💜','💙','👽','💛','💟','💚','😅','🙃','💩','😳','🙄','😑','🙇','🙎','😐','😶']\n",
    "negative_emoji = ['💥','💘','😭','😱','👎','😫','😨','😢','💀','🤔','👻','😓','💦','😤','😩','😴','💔','😒','😪','😈','😣','😮','😡','😕','😔','😠','😷','😥','😞','😲','😰','🙀','😖','😧','😟','😹','😵','😶','😯','🤒','🤕','😾','💤']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b86993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in enumerate(df_emoji['emoji']):\n",
    "    \n",
    "    for emoj in text:\n",
    "        \n",
    "        if emoj in positive_emoji:\n",
    "            df_emoji['positive_emoji'].iloc[idx] += 1\n",
    "        elif emoj in negative_emoji:\n",
    "            df_emoji['negative_emoji'].iloc[idx] += 1\n",
    "        else:\n",
    "            df_emoji['neutral_emoji'].iloc[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Voici le bilan de l'analyse de sentiments: \\n\")\n",
    "print(f\"Score positif: {df_emoji['positive_emoji'].value_counts()} \" )\n",
    "print(f\"Score négatif: {df_emoji['negative_emoji'].value_counts()} \" )\n",
    "print(f\"Score neutre:  {df_emoji['neutral_emoji'].value_counts()} \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d0aec",
   "metadata": {},
   "source": [
    "## 2.4. Suppression des hastags, url, arobase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b31e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove urls, hashtags, arobase (et emoji)\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+',' ', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#\\S+',' ', text)\n",
    "\n",
    "def remove_arobase(text):\n",
    "    return re.sub(r'@\\S+',' ', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    string = ' '.join([word for word in text if word not in emo_list])\n",
    "    return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_clean'] = df['content'].apply(remove_hashtags)\n",
    "df['content_clean'] = df['content_clean'].apply(remove_urls)\n",
    "df['content_clean'] = df['content_clean'].apply(remove_arobase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['content','content_clean']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c0efb",
   "metadata": {},
   "source": [
    "# 3. Listes de stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a4131",
   "metadata": {},
   "source": [
    "## 3.1. Module ntk: french_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Création d'une liste de stopword en français à partir de nltk\n",
    "french_stopwords_list = stopwords.words('french')\n",
    "\n",
    "# Suppression des accents\n",
    "french_stopwords_list=[unidecode(x) for x in french_stopwords_list]\n",
    "\n",
    "# ajout ukraine, guerre et annee\n",
    "french_stopwords_list.append('ukraine')\n",
    "french_stopwords_list.append('guerre')\n",
    "french_stopwords_list.append('annee')\n",
    "\n",
    "#print(f\"Ci-dessous, la liste des stopwords en français de nltk :\\n{french_stopwords_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f740f",
   "metadata": {},
   "source": [
    "## 3.2 Module stopwords: french_stopwords_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f57a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords_list_2 = get_stop_words('french')\n",
    "#print(f\"Ci-dessous, la liste des stopwords en français de stop_words :\\n{french_stopwords_list_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd96c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des accents\n",
    "\n",
    "french_stopwords_list_2=[unidecode(x) for x in french_stopwords_list_2]\n",
    "#print(f\"Ci-dessous, la liste des stopwords SANS ACCENT de stop_words :\\n{french_stopwords_list_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b99d01",
   "metadata": {},
   "source": [
    "## 3.3 Fusion des 2 listes: french_stopwords_list_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb30581",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords_list_3 = set(french_stopwords_list + french_stopwords_list_2)\n",
    "french_stopwords_list_3 = sorted (french_stopwords_list_3 )\n",
    "\n",
    "print(f\"Ci-dessous, la liste complète des stopwords en français :\\n{french_stopwords_list_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf75335",
   "metadata": {},
   "source": [
    "# 4. Prétraitement (suite) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c40c81",
   "metadata": {},
   "source": [
    "## 4.1 Prétraitement: Nettoyage complet = suppression des stopwords, etc..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d16517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction pour supprimer les stop words\n",
    "def no_stop_word(string, stopWords):\n",
    "\n",
    "    \"\"\"\n",
    "    Supprime les stop words d'un texte.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "\n",
    "    string : chaine de caractère.\n",
    "\n",
    "    stopWords : liste de mots à exclure. \n",
    "    \"\"\"\n",
    "    \n",
    "    string = ' '.join([word for word in string.split() if word not in stopWords])\n",
    "    return string\n",
    "\n",
    "# Création de la fonction de NETTOYAGE COMPLET\n",
    "def final_cleaner(pandasSeries, stopWords):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stemmatise une Series Pandas de documents \n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    \n",
    "    pandasSeries : Une Series Pandas\n",
    "\n",
    "    stemmer : Stemmer de NLTK\n",
    "    \n",
    "    stopWords : Une liste de stopWords\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"#### Nettoyage en cours ####\") \n",
    "    \n",
    "    # confirmation que chaque article est bien de type str\n",
    "    pandasSeries = pandasSeries.apply(str)\n",
    "        \n",
    "    # Passage en minuscule\n",
    "    print(\"... Passage en minuscule\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x : x.lower())\n",
    "    \n",
    "    # Suppression des accents\n",
    "    print(\"... Suppression des accents\") \n",
    "    pandasSeries = pandasSeries.apply(unidecode)\n",
    "    \n",
    "    # Détection du champs année\n",
    "    print(\"... Détection du champs année\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x : re.sub(r'[0-9]{4}', 'annee', x))\n",
    "    \n",
    "    # Suppression http\n",
    "    #print(\"... Suppression http\") \n",
    "    #pandasSeries = pandasSeries.apply(lambda x : re.sub(r'https://', ' ', x))\n",
    "    \n",
    "    # Suppression des caractères spéciaux et numériques\n",
    "    print(\"... Suppression des caractères spéciaux et numériques\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"[^a-z]+\", ' ', x))\n",
    "    \n",
    "    # Suppression des stop words\n",
    "    print(\"... Suppression des stop words\") \n",
    "    #stopWords = [unidecode(sw) for sw in stopWords]\n",
    "    pandasSeries = pandasSeries.apply(lambda x : no_stop_word(x, stopWords))\n",
    " \n",
    "    print(\"#### Nettoyage OK! ####\")\n",
    "\n",
    "    return pandasSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15658f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du nettoyage final\n",
    "df['content_clean_final'] = final_cleaner(df['content_clean'], french_stopwords_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba31253",
   "metadata": {},
   "source": [
    "## 4.2 STEMMATISATION et LEMMATISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aff813",
   "metadata": {},
   "source": [
    "### 4.2.1 STEMMATISATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d27a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de STEMMATISATION\n",
    "\n",
    "def stemmatise_text(text,stemmer):\n",
    "\n",
    "    \"\"\"\n",
    "    Stemmatise un texte : Ramène les mots d'un texte à leur racine (peut créer des mots qui n'existe pas).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "\n",
    "    text : Chaine de caractères.\n",
    "\n",
    "    stemmer : Stemmer de NLTK.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93da3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise un stemmer NLTK\n",
    "stemmer = SnowballStemmer('french')\n",
    "\n",
    "\n",
    "# Application de la fonction stemmatise_text\n",
    "df['content_stem'] = df['content_clean_final'].apply(lambda x : stemmatise_text(x,stemmer))\n",
    "\n",
    "df[['content_clean_final', 'content_stem']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17df0a3",
   "metadata": {},
   "source": [
    "### 4.2.2 LEMMATISATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une fonction de LEMMATISATION\n",
    "\n",
    "def lemmatise_text(text):\n",
    "\n",
    "    \"\"\"\n",
    "   Lemmatise un texte \n",
    "    Paramètres\n",
    "    ----------\n",
    "\n",
    "    text : Chaine de caractères.\n",
    "\n",
    "    lemmer : lemmer de simplema\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([simplemma.lemmatize(word, lang='fr') for word in text.split()])\n",
    "\n",
    "# Application de la fonction lemmatise_text\n",
    "df['content_lem'] = df['content_clean_final'].apply(lambda x : lemmatise_text(x))\n",
    "\n",
    "df[['content_stem', 'content_lem']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53932e31",
   "metadata": {},
   "source": [
    "### 4.2.3 Filtrage des tweets qui contiennent trop peu de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341242b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille avant filtrage\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage \n",
    "df = df[df['content_lem'].str.len() >= 3]\n",
    "# Taille après filtrage\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaaf55c",
   "metadata": {},
   "source": [
    "## 4.3. Visualisation des MOTS LES PLUS UTILISES après traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fa3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une variable contenant le nombre de \"mots\" de chaque article\n",
    "df['nb_words_lem'] = df['content_lem'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Affichage du dataframe df\n",
    "#df.head()\n",
    "\n",
    "# Répartition des tweets en fonction du nombre de mots\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.hist(df['nb_words_lem'], bins=30, color='b', edgecolor='k')\n",
    "_ = ax.set_xlabel('Number of words after preprocessing')\n",
    "_ = ax.set_ylabel('Number of tweets')\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe01400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nb_words_lem'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['nb_words_lem'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f25672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['nb_words_lem']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "import wordcloud \n",
    "\n",
    "# Join the different processed tweets together.\n",
    "long_string = ' '.join(df['content_stem'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wc = wordcloud.WordCloud(width=400,\n",
    "                      height=330,\n",
    "                      max_words=50,\n",
    "                      colormap='tab20c',\n",
    "                      collocations=True)\n",
    "\n",
    "# Generate a word cloud\n",
    "wc.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.title('Words Clouds', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3394859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher les mots les plus utilisés \n",
    "def print_words(df , col, nb_words):\n",
    "    \n",
    "    \"\"\"\n",
    "   print les max_words mots les plus fréquemment utilisés par cluster\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame Pandas\n",
    "    \n",
    "    col : La série de df à analyser (après pré-processing)\n",
    "\n",
    "    nb_words : nombre de mots\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    data = df[col]\n",
    "        \n",
    "    long_string = ' '.join(data)\n",
    "        \n",
    "    my_counts =  Counter(re.findall('\\w+', long_string))\n",
    "  \n",
    "    most_occur = my_counts.most_common(nb_words)\n",
    "  \n",
    "    #print(f\"Top {nb_words} :\\n {most_occur}.\")\n",
    "    \n",
    "    return most_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list=print_words(df , 'content_lem', 20)\n",
    "df_top = pd.DataFrame(top_list, columns= ['Word','Count'])\n",
    "df_top.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f3ae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(df_top['Word'], df_top['Count'])\n",
    "_ = ax.set_xticklabels(df_top['Word'], rotation=45)\n",
    "_ = ax.set_xlabel('Mot')\n",
    "_ = ax.set_ylabel('Nombre d\\'occurences')\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85aa54",
   "metadata": {},
   "source": [
    "## 4.4. Analyse des hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67647c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de tweets pour lesquels il n'y a pas de hastag\n",
    "df['hashtags'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ca68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de ces tweets qui sont peu nombreux\n",
    "df.dropna(subset=['hashtags'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c749b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui transforme les hashtags en une liste de mots\n",
    "def list_hashtags(list_txt):\n",
    "    if len(list_txt)==1:\n",
    "        return list_txt[0]\n",
    "    else: \n",
    "        return ' '.join(list_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d53d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_hashtags([df['hashtags'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags_clean'] = df['hashtags'].apply(lambda x : list_hashtags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0683e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['hashtags', 'hashtags_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different processed tweets together.\n",
    "long_string_hashtags = ' '.join(df['hashtags_clean'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wc = wordcloud.WordCloud(width=400,\n",
    "                      height=330,\n",
    "                      max_words=50,\n",
    "                      colormap='tab20c',\n",
    "                      collocations=True)\n",
    "\n",
    "# Generate a word cloud\n",
    "wc.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.title('Words Clouds Hashtags', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a65a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list_hashtags=print_words(df , 'hashtags_clean', 20)\n",
    "df_top_hashtags = pd.DataFrame(top_list, columns= ['Word','Count'])\n",
    "df_top_hashtags.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca92e0",
   "metadata": {},
   "source": [
    "L'analyse des mots les plus utilisés dans les hashtags et les tweets donne à peu près la même chose..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c25ad5",
   "metadata": {},
   "source": [
    "# 5. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfba7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des tweets qui contiennent moins de 3 mots après lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a818e8",
   "metadata": {},
   "source": [
    "## 5.1 Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3309c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoriseur TFIDF: on ignore les mots présents dans plus de 50% des tweets ou dans moins de 1000 tweets\n",
    "# On ne prend en compte que les bigrams et lres trigrams.\n",
    "tfidf = TfidfVectorizer(max_df=0.5,\n",
    "                        min_df=2000,\n",
    "                        ngram_range=(1,3))\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afe30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoriseur COUNT\n",
    "#count = CountVectorizer(min_df=1000,\n",
    "                        #max_df=0.5,\n",
    "                        #ngram_range=(1, 3), # sélection bigrammes\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf.fit_transform([x for x in df[\"content_lem\"]])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_matrix = count.fit_transform([x for x in df[\"content_lem\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(count_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words_tfidf = tfidf.get_feature_names()\n",
    "#type(list_words_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a8e0",
   "metadata": {},
   "source": [
    "https://kavita-ganesan.com/hashingvectorizer-vs-countvectorizer/#.YzVS_YRBwuW\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c681748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wc = wordcloud.WordCloud(width=400,\n",
    "                      height=330,\n",
    "                      max_words=50,\n",
    "                      colormap='tab20c',\n",
    "                      collocations=True)\n",
    "\n",
    "# Generate a word cloud\n",
    "wc.generate(' '.join(list_words_tfidf))\n",
    "\n",
    "# Visualize the word cloud\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.title('Words Clouds TFIDF', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances =[]\n",
    "K = range(1,30,2)\n",
    "for k in K:\n",
    "    km =KMeans(n_clusters =k)\n",
    "    km =km.fit(tfidf_matrix)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dcb43",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f191236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de kmeans puis création d'une colonne cluster\n",
    "km = KMeans(\n",
    "        n_clusters=10,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=5)\n",
    "\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix or count_matrix\n",
    "km.fit_transform(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Create a column cluster to denote the generated cluster for each tweet\n",
    "df[\"cluster\"] = clusters\n",
    "\n",
    "# Display number of tweets per cluster \n",
    "df['cluster'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_words_clusters(df , col, col_clus,  nb_cluster, nb_words):\n",
    "    \n",
    "    \"\"\"\n",
    "   print les max_words mots les plus fréquemment utilisés par cluster\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame Pandas\n",
    "    \n",
    "    col : La série de df à analyser (sur lesquelles les clusters ont été calculés)\n",
    "\n",
    "    nb_cluster : nombre de clusters à prendre en compte\n",
    "    \n",
    "    nb_words : nombre de mots\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(nb_cluster):\n",
    "        \n",
    "        data = df[df[col_clus] == i][col]\n",
    "        \n",
    "        long_string = ' '.join(data)\n",
    "        \n",
    "        my_counts =  Counter(re.findall('\\w+', long_string))\n",
    "  \n",
    "        most_occur = my_counts.most_common(nb_words)\n",
    "  \n",
    "        print(f\"Top {nb_words} du cluster n = {i+1} :\\n {most_occur}.\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des différents types de tweets suivant les clusters:\n",
    "# Join the different processed titles together.\n",
    "\n",
    "def plot_words_clusters(df , col, col_clus,  nb_cluster, max_words):\n",
    "    \n",
    "    \"\"\"\n",
    "   Trace un words_clouds pour chaque cluster \n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame Pandas\n",
    "    \n",
    "    col : La série de df à analyser (sur lesquelles les clusters ont été calculés)\n",
    "\n",
    "    nb_cluster : nombre de clusters à prendre en compte\n",
    "    \n",
    "    max_words : nombre max de mots\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(nb_cluster):\n",
    "        \n",
    "        data = df[df[col_clus] == i][col]\n",
    "        \n",
    "        long_string = ' '.join(data)\n",
    "\n",
    "        # Create a WordCloud object\n",
    "        wc = wordcloud.WordCloud(width=400,\n",
    "                                height=330,\n",
    "                                max_words=max_words,\n",
    "                                colormap='tab20c',\n",
    "                                collocations=True)\n",
    "\n",
    "        # Generate a word cloud\n",
    "        wc.generate(long_string)\n",
    "\n",
    "        # Visualize the word cloud\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.imshow(wc)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Words Clouds pour le cluster n={i+1}', fontsize=13)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words_clusters(df,\"content_lem\", \"cluster\", 10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390a246",
   "metadata": {},
   "source": [
    "## 5.3 Kmeans après réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536a9a8",
   "metadata": {},
   "source": [
    "### 5.2.1. Kmeans avec réduction de dimension: avec TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction de dimension:\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=50), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(tfidf_matrix)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recherche du nombre de clusters optimum: méthode du coude\n",
    "\n",
    "Sum_of_squared_distances =[]\n",
    "K = range(1,40)\n",
    "for k in K:\n",
    "    km =KMeans(n_clusters =k)\n",
    "    km =km.fit(X_lsa)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ea094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de kmeans après réduction de dimension\n",
    "\n",
    "km = KMeans(\n",
    "        n_clusters=10,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=1)\n",
    "\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix or count_matrix\n",
    "km.fit_transform(X_lsa)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Create a column cluster to denote the generated cluster for each tweet\n",
    "df[\"cluster_lsa\"] = clusters\n",
    "\n",
    "# Display number of tweets per cluster \n",
    "df['cluster_lsa'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words_clusters(df,\"content_lem\", \"cluster_lsa\", 10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ce33b",
   "metadata": {},
   "source": [
    "# 6. Recherche de topics: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f47fc",
   "metadata": {},
   "source": [
    "## 6.1 LDA: First method (gensim+doc2bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4bfc9",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://www.kaggle.com/code/vukglisovic/classification-combining-lda-and-word2vec\n",
    "\n",
    "https://towardsdatascience.com/lda-topic-modeling-with-tweets-deff37c0e131\n",
    "\n",
    "https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know\n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse statistique\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533efffd",
   "metadata": {},
   "source": [
    "Certains tweets ne contiennent aucun mot après traitement: on les supprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_null = df[df['nb_words_lem']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [text.split() for text in df_non_null['content_lem']]\n",
    "all_words = [y for x in all_words for y in x]\n",
    "all_words_unique= list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e5bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Le corpus comporte {len(all_words_unique)} mots différents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ec449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = sorted(all_words_unique)\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots les plus fréquemment utilisés\n",
    "word_freq = FreqDist(all_words)\n",
    "\n",
    "#word_freq.most_common(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve word and count from FreqDist tuples\n",
    "\n",
    "most_common_count = [x[1] for x in word_freq.most_common(30)]\n",
    "most_common_word = [x[0] for x in word_freq.most_common(30)]\n",
    "\n",
    "#create dictionary mapping of word count\n",
    "top_30_dictionary = dict(zip(most_common_word, most_common_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a879f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(colormap = 'Accent', background_color = 'black').generate_from_frequencies(top_30_dictionary)\n",
    "\n",
    "#plot with matplotlib\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('top_30_cloud.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_text(text):\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64123ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_null['stem_tokens']=df_non_null['content_stem'].apply(lambda x : token_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tokens = df_non_null['stem_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f028dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcae034",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(documents=df_non_null['stem_tokens'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le dictionnaire comporte {} mots.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_above=0.75, no_below=1000)\n",
    "\n",
    "dictionary.compactify()  # Reindexes the remaining words after filtering\n",
    "print(\"Après suppression des extrêmes, il reste {} mots.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318748ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Voici les identifiants des mots de dictionnary après suppression outliers:\\n\\n {dictionary.token2id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b31fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "tweets_bow = [dictionary.doc2bow(tweet) for tweet in df_non_null['stem_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output will contain a vector for each tweet, in the form of (word id, frequency of word occurrence in document)\n",
    "# Les 3 premiers tweets:\n",
    "tweets_bow[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0619bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA: 5 topics\n",
    "\n",
    "k = 5\n",
    "tweets_lda = LdaModel(tweets_bow,\n",
    "                      num_topics = k,\n",
    "                      id2word = dictionary,\n",
    "                      random_state = 1,\n",
    "                      passes=10)\n",
    "\n",
    "tweets_lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd81a2d",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensimvis.prepare(tweets_lda, tweets_bow, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d336c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=tweets_lda, texts=df_non_null['stem_tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7b55a",
   "metadata": {},
   "source": [
    "## 6.2 LDA: Seconde méthode (sklearn avec count_matrix)\n",
    "https://github.com/bicachu/topic-modeling-health-tweets/blob/master/notebooks/LDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define target number of topics\n",
    "n_topics = 10\n",
    "\n",
    "# Fit model\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', batch_size=10000, \n",
    "                                          random_state=0, learning_decay=0.5, verbose=0)\n",
    "# Create topic matrix\n",
    "lda_topic_matrix = lda_model.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6347d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c654ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_keys = get_keys(lda_topic_matrix)\n",
    "lda_categories, lda_counts = keys_to_counts(lda_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b399a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Display top 10 words for each topic\n",
    "top_n_words_lda = get_top_n_words(10, lda_keys, count_matrix, count) \n",
    "for i in range(len(top_n_words_lda)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #plt.xticks(rotation= )\n",
    "    #fig.autofmt_xdate(rotation= )\n",
    "    #ax.set_xticklabels(xlabels, rotation= )\n",
    "    #plt.setp(ax.get_xticklabels(), rotation=)\n",
    "    #ax.tick_params(axis='x', labelrotation= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2465c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tweet counts by topics\n",
    "top_5_words = get_top_n_words(5, lda_keys, count_matrix, count) \n",
    "\n",
    "labels = ['Topic {}: \\n'.format(i) + top_5_words[i] for i in lda_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lda_categories, lda_counts);\n",
    "ax.set_xticks(lda_categories);\n",
    "ax.set_xticklabels(labels, rotation=45);\n",
    "ax.set_title('LDA topic counts');\n",
    "ax.set_ylabel('Number of tweets');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_topic_matrix_sample =  lda_topic_matrix_test.sample(n=10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df89fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_topic_matrix_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aca89e",
   "metadata": {},
   "source": [
    "# 7. Utilisation de Word2Vec + T SNE visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e77fc",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/achintyatripathi/gensim-word2vec-usage-with-t-sne-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e41ebc",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases,Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phrases() takes a list of list of words as input\n",
    "sent = [row.split() for row in df['content_lem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(sent,min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6357d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90bd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Voici la liste des mots et des bigrams avec le nombre d'occurences:\\n\\n {word_freq}.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classement du mot le plus utilisé au mot le moins utilisé\n",
    "sorted(word_freq, key=word_freq.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ab569",
   "metadata": {},
   "source": [
    "\n",
    "The parameters :\n",
    "\n",
    "    min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "    window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "\n",
    "    vector_size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "    negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "    workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buil the vocabulary\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacdcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"victime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the array\n",
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89630ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=20).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad41d61",
   "metadata": {},
   "source": [
    " We will look at the relationships between a query word (in **red**), its most similar words in the model (in **blue**), and other words from the vocabulary (in **green**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'victime',[t[0] for t in w2v_model.wv.most_similar(positive=[\"victime\"], topn=20)][10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLUSTERS=3\n",
    "\n",
    "\n",
    "X=w2v_model.wv.vectors\n",
    "\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "print (assigned_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
