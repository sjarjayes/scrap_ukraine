{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d63d435",
   "metadata": {},
   "source": [
    "Ce notebook prÃ©sente diffÃ©rentes algorithmes de Machine Learning appliquÃ©s Ã  des tweets.\n",
    "Il s'agit d'un projet d'Ã©tude rÃ©alisÃ© dans le cadre du DU Data Analysis de la Sorbonne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f5c9e6-5e31-47fc-8a07-996f4fe08c2c",
   "metadata": {},
   "source": [
    "# 1. Import des donnÃ©es\n",
    "* https://mihaelagrigore.medium.com/scraping-historical-tweets-without-a-twitter-developer-account-79a2c61f76ab\n",
    "* https://github.com/JustAnotherArchivist/snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f63526a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yellowbrick\n",
      "  Downloading yellowbrick-1.5-py3-none-any.whl (282 kB)\n",
      "     -------------------------------------- 282.6/282.6 kB 5.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.7.3)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (3.5.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (1.0.2)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (21.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->yellowbrick) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n",
      "Installing collected packages: yellowbrick\n",
      "Successfully installed yellowbrick-1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1889ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 15.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.21.5)\n",
      "Requirement already satisfied: numexpr in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: future in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sylvi\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis) (65.3.0)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sylvi\\anaconda3\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.4)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (pyproject.toml): started\n",
      "  Building wheel for pyLDAvis (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136882 sha256=9e43054590da8d3a285a2de0520369af1f1dfb74fb95af320bbd7033e7de3110\n",
      "  Stored in directory: c:\\users\\sylvi\\appdata\\local\\pip\\cache\\wheels\\57\\a4\\86\\d10c6c2e0bf149fbc0afb0aa5a6528ac35b30a133a0270c477\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=28447f0db4f9f157d1fa51e7cbf4c81634c46de822a2c87046db843363e24d76\n",
      "  Stored in directory: c:\\users\\sylvi\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: funcy, sklearn, pyLDAvis\n",
      "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3275c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simplemma\n",
      "  Downloading simplemma-0.9.0-py3-none-any.whl (76.2 MB)\n",
      "     --------------------------------------- 76.2/76.2 MB 32.8 MB/s eta 0:00:00\n",
      "Installing collected packages: simplemma\n",
      "Successfully installed simplemma-0.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install simplemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63efff1b-6e28-4376-afaa-cfb1d4d9d83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sylvi\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:400: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\Users\\sylvi\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\sylvi\\anaconda3\\lib\\site-packages\\yellowbrick\\style\\colors.py:35: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  mpl_ge_150 = LooseVersion(mpl.__version__) >= \"1.5.0\"\n",
      "C:\\Users\\sylvi\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "C:\\Users\\sylvi\\anaconda3\\lib\\site-packages\\yellowbrick\\style\\rcmod.py:31: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  mpl_ge_150 = LooseVersion(mpl.__version__) >= \"1.5.0\"\n",
      "C:\\Users\\sylvi\\AppData\\Roaming\\Python\\Python39\\site-packages\\setuptools\\_distutils\\version.py:346: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import uuid\n",
    "\n",
    "from IPython.display import display_javascript, display_html, display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pour le prÃ© processing\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Pour la dataviz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Les bigrammes\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Pour la vectorisation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Pour la modÃ©lisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "\n",
    "import emojis\n",
    "\n",
    "sns.set()\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "import simplemma\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "\n",
    "from nltk.cluster import KMeansClusterer\n",
    "\n",
    "\n",
    "pd.set_option('display.min_rows', 50)\n",
    "pd.options.display.max_colwidth = 150\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # setting ignore as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6deac8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choix ancien fichier=0, Choix nouvelle recherche=1 : 0\n"
     ]
    }
   ],
   "source": [
    "# Choix entre un fichier existant ou une nouvelle recherche\n",
    "choix_data=int(input('Choix ancien fichier=0, Choix nouvelle recherche=1 : '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc1f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if choix_data==1:\n",
    "\n",
    "    # Scrap d'un nouveau fichier et Sauvegarde sous format json\n",
    "    json_filename = 'Ukraine-query-tweets-light.json'\n",
    "\n",
    "    #Using the OS library to call CLI commands in Python\n",
    "    os.system(f'snscrape --max-results 50000 --jsonl --progress --since 2022-02-24 twitter-search \"#Ukraine lang:fr until:2022-08-18\" > {json_filename}')\n",
    "\n",
    "elif choix_data==0:\n",
    "\n",
    "    # Utilisation d'un fichier json existant\n",
    "    json_filename = 'Ukraine-query-tweets-v2.json'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb5057e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>inReplyToTweetId</th>\n",
       "      <th>inReplyToUser</th>\n",
       "      <th>mentionedUsers</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/M_Degage/status/1560052163212152837</td>\n",
       "      <td>2022-08-17 23:53:14+00:00</td>\n",
       "      <td>ğŸ”#INFO Ã  #RT ğŸ™ğŸ’– \\nğŸ‡«ğŸ‡· #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\nğŸš¨Jour_175 #GUERRE #Ukraine + #OTAN &amp;gt;&amp;gt; #RUSSIE + #...</td>\n",
       "      <td>ğŸ”#INFO Ã  #RT ğŸ™ğŸ’– \\nğŸ‡«ğŸ‡· #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\nğŸš¨Jour_175 #GUERRE #Ukraine + #OTAN &amp;gt;&amp;gt; #RUSSIE + #...</td>\n",
       "      <td>1560052163212152837</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': 'ğŸ‡«ğŸ‡·FIL sous MACRO. Dictateur-qui-ment...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Photo', 'previewUrl': 'https://pbs.twimg.com/media/FaZidccWQAAiiDX?format=jpg&amp;name=small', 'fullUrl': 'https:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': 'ğŸ‡«ğŸ‡·FIL sous MACRO. Dictateur-qui-men...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[INFO, RT, FR, RU, eZ, GJ, JB, lr, Zemmour, Zozz, Patriotes, JambonBeurre, GUERRE, Ukraine, OTAN, RUSSIE, Donbass, DerniÃ¨resInfos]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/millimagino/status/1560051755286757377</td>\n",
       "      <td>2022-08-17 23:51:37+00:00</td>\n",
       "      <td>#Ukraine / Centrale nuclÃ©aire de Zaporijjia: Kiev affirme qu'il faut se \"prÃ©parer Ã  tous les scÃ©narios\" https://t.co/PA2qLQUUEb via @BFMTV</td>\n",
       "      <td>#Ukraine / Centrale nuclÃ©aire de Zaporijjia: Kiev affirme qu'il faut se \"prÃ©parer Ã  tous les scÃ©narios\" bfmtv.com/international/â€¦ via @BFMTV</td>\n",
       "      <td>1560051755286757377</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'millimagino', 'id': 196342852, 'displayname': 'ğ“¢ğ“®ğ“»ğ“°ğ“® (Gros Loup) â™ª', 'description': 'ğŸ¸ â—® âœª...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': 'BFMTV', 'id': 133663801, 'displayname': 'BFMTV', 'description': None, 'rawDescription': N...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/Lejojo66/status/1560049701315018752</td>\n",
       "      <td>2022-08-17 23:43:27+00:00</td>\n",
       "      <td>@WAW_AgainstWar Je me demande ce quâ€™ils vont dire aux peuples russe ses prÃ©sentateurs a la coâ€¦quand la Poutine va perdre Kherson et la CrimÃ©e. En ...</td>\n",
       "      <td>@WAW_AgainstWar Je me demande ce quâ€™ils vont dire aux peuples russe ses prÃ©sentateurs a la coâ€¦quand la Poutine va perdre Kherson et la CrimÃ©e. En ...</td>\n",
       "      <td>1560049701315018752</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'Lejojo66', 'id': 1432071090545831944, 'displayname': 'Jojo', 'description': '', 'rawDescri...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.559957e+18</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': 'â‚©AW: War Against War', 'descri...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': 'â‚©AW: War Against War', 'descr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/UkrinformFra/status/1560048041670975488</td>\n",
       "      <td>2022-08-17 23:36:52+00:00</td>\n",
       "      <td>Guerre en Ukraine : Deux civils tuÃ©s et sept blessÃ©s dans la rÃ©gion de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...</td>\n",
       "      <td>Guerre en Ukraine : Deux civils tuÃ©s et sept blessÃ©s dans la rÃ©gion de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...</td>\n",
       "      <td>1560048041670975488</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'UkrinformFra', 'id': 835057652937916416, 'displayname': 'Ukrinform en franÃ§ais', 'descript...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine, UkraineRussie, UkraineInvasion, Russie, GuerreEnUkraine, guerreUkraine, Donetsk]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/ErikBoesch/status/1560047415612379136</td>\n",
       "      <td>2022-08-17 23:34:22+00:00</td>\n",
       "      <td>@24hPujadas @trinquand Berlin a-t-il connaissance de la prÃ©sence en #Ukraine de matÃ©riel militaire qui pourrait avoir Ã©tÃ© remis Ã  #Kiev par des Br...</td>\n",
       "      <td>@24hPujadas @trinquand Berlin a-t-il connaissance de la prÃ©sence en #Ukraine de matÃ©riel militaire qui pourrait avoir Ã©tÃ© remis Ã  #Kiev par des Br...</td>\n",
       "      <td>1560047415612379136</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': 'ErikBoesch', 'id': 1946616553, 'displayname': 'Erik P. Alfred', 'description': 'Vive l'ami...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1.559937e+18</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None, ...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None,...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Ukraine, Kiev]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _type  \\\n",
       "0  snscrape.modules.twitter.Tweet   \n",
       "1  snscrape.modules.twitter.Tweet   \n",
       "2  snscrape.modules.twitter.Tweet   \n",
       "3  snscrape.modules.twitter.Tweet   \n",
       "4  snscrape.modules.twitter.Tweet   \n",
       "\n",
       "                                                           url  \\\n",
       "0      https://twitter.com/M_Degage/status/1560052163212152837   \n",
       "1   https://twitter.com/millimagino/status/1560051755286757377   \n",
       "2      https://twitter.com/Lejojo66/status/1560049701315018752   \n",
       "3  https://twitter.com/UkrinformFra/status/1560048041670975488   \n",
       "4    https://twitter.com/ErikBoesch/status/1560047415612379136   \n",
       "\n",
       "                       date  \\\n",
       "0 2022-08-17 23:53:14+00:00   \n",
       "1 2022-08-17 23:51:37+00:00   \n",
       "2 2022-08-17 23:43:27+00:00   \n",
       "3 2022-08-17 23:36:52+00:00   \n",
       "4 2022-08-17 23:34:22+00:00   \n",
       "\n",
       "                                                                                                                                                 content  \\\n",
       "0  ğŸ”#INFO Ã  #RT ğŸ™ğŸ’– \\nğŸ‡«ğŸ‡· #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\nğŸš¨Jour_175 #GUERRE #Ukraine + #OTAN &gt;&gt; #RUSSIE + #...   \n",
       "1             #Ukraine / Centrale nuclÃ©aire de Zaporijjia: Kiev affirme qu'il faut se \"prÃ©parer Ã  tous les scÃ©narios\" https://t.co/PA2qLQUUEb via @BFMTV   \n",
       "2  @WAW_AgainstWar Je me demande ce quâ€™ils vont dire aux peuples russe ses prÃ©sentateurs a la coâ€¦quand la Poutine va perdre Kherson et la CrimÃ©e. En ...   \n",
       "3  Guerre en Ukraine : Deux civils tuÃ©s et sept blessÃ©s dans la rÃ©gion de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...   \n",
       "4  @24hPujadas @trinquand Berlin a-t-il connaissance de la prÃ©sence en #Ukraine de matÃ©riel militaire qui pourrait avoir Ã©tÃ© remis Ã  #Kiev par des Br...   \n",
       "\n",
       "                                                                                                                                         renderedContent  \\\n",
       "0  ğŸ”#INFO Ã  #RT ğŸ™ğŸ’– \\nğŸ‡«ğŸ‡· #FR #RU #eZ #GJ #JB #lr \\n#Zemmour #Zozz #Patriotes #JambonBeurre \\n\\nğŸš¨Jour_175 #GUERRE #Ukraine + #OTAN &gt;&gt; #RUSSIE + #...   \n",
       "1           #Ukraine / Centrale nuclÃ©aire de Zaporijjia: Kiev affirme qu'il faut se \"prÃ©parer Ã  tous les scÃ©narios\" bfmtv.com/international/â€¦ via @BFMTV   \n",
       "2  @WAW_AgainstWar Je me demande ce quâ€™ils vont dire aux peuples russe ses prÃ©sentateurs a la coâ€¦quand la Poutine va perdre Kherson et la CrimÃ©e. En ...   \n",
       "3  Guerre en Ukraine : Deux civils tuÃ©s et sept blessÃ©s dans la rÃ©gion de Donetsk \\n#Ukraine #UkraineRussie #UkraineInvasion #Russie #GuerreEnUkraine...   \n",
       "4  @24hPujadas @trinquand Berlin a-t-il connaissance de la prÃ©sence en #Ukraine de matÃ©riel militaire qui pourrait avoir Ã©tÃ© remis Ã  #Kiev par des Br...   \n",
       "\n",
       "                    id  \\\n",
       "0  1560052163212152837   \n",
       "1  1560051755286757377   \n",
       "2  1560049701315018752   \n",
       "3  1560048041670975488   \n",
       "4  1560047415612379136   \n",
       "\n",
       "                                                                                                                                                    user  \\\n",
       "0  {'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': 'ğŸ‡«ğŸ‡·FIL sous MACRO. Dictateur-qui-ment...   \n",
       "1  {'_type': 'snscrape.modules.twitter.User', 'username': 'millimagino', 'id': 196342852, 'displayname': 'ğ“¢ğ“®ğ“»ğ“°ğ“® (Gros Loup) â™ª', 'description': 'ğŸ¸ â—® âœª...   \n",
       "2  {'_type': 'snscrape.modules.twitter.User', 'username': 'Lejojo66', 'id': 1432071090545831944, 'displayname': 'Jojo', 'description': '', 'rawDescri...   \n",
       "3  {'_type': 'snscrape.modules.twitter.User', 'username': 'UkrinformFra', 'id': 835057652937916416, 'displayname': 'Ukrinform en franÃ§ais', 'descript...   \n",
       "4  {'_type': 'snscrape.modules.twitter.User', 'username': 'ErikBoesch', 'id': 1946616553, 'displayname': 'Erik P. Alfred', 'description': 'Vive l'ami...   \n",
       "\n",
       "   replyCount  retweetCount  likeCount  ...  \\\n",
       "0           0             6          8  ...   \n",
       "1           0             0          0  ...   \n",
       "2           1             0          0  ...   \n",
       "3           0             0          0  ...   \n",
       "4           0             0          0  ...   \n",
       "\n",
       "                                                                                                                                                   media  \\\n",
       "0  [{'_type': 'snscrape.modules.twitter.Photo', 'previewUrl': 'https://pbs.twimg.com/media/FaZidccWQAAiiDX?format=jpg&name=small', 'fullUrl': 'https:...   \n",
       "1                                                                                                                                                   None   \n",
       "2                                                                                                                                                   None   \n",
       "3                                                                                                                                                   None   \n",
       "4                                                                                                                                                   None   \n",
       "\n",
       "   retweetedTweet quotedTweet inReplyToTweetId  \\\n",
       "0             NaN        None              NaN   \n",
       "1             NaN        None              NaN   \n",
       "2             NaN        None     1.559957e+18   \n",
       "3             NaN        None              NaN   \n",
       "4             NaN        None     1.559937e+18   \n",
       "\n",
       "                                                                                                                                           inReplyToUser  \\\n",
       "0                                                                                                                                                   None   \n",
       "1                                                                                                                                                   None   \n",
       "2  {'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': 'â‚©AW: War Against War', 'descri...   \n",
       "3                                                                                                                                                   None   \n",
       "4  {'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None, ...   \n",
       "\n",
       "                                                                                                                                          mentionedUsers  \\\n",
       "0  [{'_type': 'snscrape.modules.twitter.User', 'username': 'M_Degage', 'id': 1491224931354431490, 'displayname': 'ğŸ‡«ğŸ‡·FIL sous MACRO. Dictateur-qui-men...   \n",
       "1  [{'_type': 'snscrape.modules.twitter.User', 'username': 'BFMTV', 'id': 133663801, 'displayname': 'BFMTV', 'description': None, 'rawDescription': N...   \n",
       "2  [{'_type': 'snscrape.modules.twitter.User', 'username': 'WAW_AgainstWar', 'id': 1504387171335094276, 'displayname': 'â‚©AW: War Against War', 'descr...   \n",
       "3                                                                                                                                                   None   \n",
       "4  [{'_type': 'snscrape.modules.twitter.User', 'username': '24hPujadas', 'id': 765957157175058432, 'displayname': '24h Pujadas', 'description': None,...   \n",
       "\n",
       "  coordinates place  \\\n",
       "0        None  None   \n",
       "1        None  None   \n",
       "2        None  None   \n",
       "3        None  None   \n",
       "4        None  None   \n",
       "\n",
       "                                                                                                                             hashtags  \\\n",
       "0  [INFO, RT, FR, RU, eZ, GJ, JB, lr, Zemmour, Zozz, Patriotes, JambonBeurre, GUERRE, Ukraine, OTAN, RUSSIE, Donbass, DerniÃ¨resInfos]   \n",
       "1                                                                                                                           [Ukraine]   \n",
       "2                                                                                                                           [Ukraine]   \n",
       "3                                          [Ukraine, UkraineRussie, UkraineInvasion, Russie, GuerreEnUkraine, guerreUkraine, Donetsk]   \n",
       "4                                                                                                                     [Ukraine, Kiev]   \n",
       "\n",
       "   cashtags  \n",
       "0      None  \n",
       "1      None  \n",
       "2      None  \n",
       "3      None  \n",
       "4      None  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# CrÃ©ation d'un dataframe df contentant les tweets\n",
    "\n",
    "df = pd.read_json(json_filename, lines=True)\n",
    "\n",
    "# Sauvegarde d'une copie\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Visualisation de df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "783b419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('scrap_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa987a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'projet_scrap_tweet_27102022_light.ipynb',\n",
       " 'scarp_tweet_ukraine_29092022.ipynb',\n",
       " 'scrap_tweets.csv',\n",
       " 'scrap_tweet_ukraine_01102022.ipynb',\n",
       " 'scrap_tweet_ukraine_20092022.ipynb',\n",
       " 'Ukraine-query-tweets-light.json',\n",
       " 'Ukraine-query-tweets-v2.json']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593d26f8",
   "metadata": {},
   "source": [
    "# 2. PremiÃ¨res analyses: Ã©volution du nombre de tweets dans le temps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700a647",
   "metadata": {},
   "source": [
    "## 2.1 Evolution du nombre de tweets dans le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c18fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation de df2: 2 colonnes= day + count=nbre de tweets par jour\n",
    "df['day'] = df['date'].dt.strftime('%D') \n",
    "df['week_number'] = df['date'].dt.strftime('%V') \n",
    "df2=df.groupby(['day']).size().reset_index(name='counts')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution du nombre de tweets par jour\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "x = df2['day']\n",
    "y = df2['counts']\n",
    "plt.plot(x, y)\n",
    "_ = ax.set_xticks(x[::5])\n",
    "_ = ax.set_xticklabels(x[::5], rotation=45)\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "#_ = plt.xlabel('Jour')\n",
    "_ = plt.ylabel('Nombre de tweets par jour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation de df3: 2 colonnes= semaine + count=nbre de tweets par semaine\n",
    "df3=df.groupby(['week_number']).size().reset_index(name='counts')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolution du nombre de tweets par semaine\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(df3['week_number'], df3['counts'])\n",
    "_ = ax.set_xlabel('Week Number')\n",
    "_ = ax.set_ylabel('Number of tweets')\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5b0ac6",
   "metadata": {},
   "source": [
    "## 2.2 Nombre de mots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths = np.array(list(map(len, df['content'].str.split(' '))))\n",
    "\n",
    "print(f\"Le nombre moyen de mots par tweet est : {int(np.mean(document_lengths))}.\")\n",
    "\n",
    "\n",
    "print(f\"Le nombre minimum de mots par tweet est: {min(document_lengths)}.\")\n",
    "print(f\"Le nombre maximum de mots par tweet est: {max(document_lengths)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "_=ax.set_title(\"Distribution of number of words before preprocessing\", fontsize=16)\n",
    "_=ax.set_xlabel(\"Number of words\")\n",
    "_=sns.distplot(document_lengths, bins=50, ax=ax)\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b56ff",
   "metadata": {},
   "source": [
    "## 2.3. Analyse des emoji pour prÃ©parer une analyse de sentiments (non traitÃ©e ici)\n",
    "* https://emojis.readthedocs.io/en/latest/api.html#module-emojis\n",
    "* https://www.kaggle.com/code/infamouscoder/emoji-sentiment-features\n",
    "* https://www.kaggle.com/code/infamouscoder/emoji-sentiment-features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a93fdc",
   "metadata": {},
   "source": [
    "### 2.3.1. Analyse globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d0848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab800ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_emoji(my_pd):\n",
    "    \"\"\"\n",
    "    CrÃ©e une liste des labels des emojis en francais.\n",
    "\n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "\n",
    "    my_list : liste d'emojis.\n",
    "\n",
    "    \"\"\"\n",
    "    col_emo=[]\n",
    "    my_set=emojis.get(my_pd)\n",
    "    #for emo in my_set:\n",
    "    #    val = emoji.demojize(emo, language='fr').split(':')[1]\n",
    "    #    col_emo.append(val)\n",
    "    return list(my_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation d'une colonne qui contient tous les Ã©mojis des tweets\n",
    "df['emoji']=df['content'].apply((lambda x : create_column_emoji(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83df78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Le % de tweets comportant des emojis est: {np.ceil(100*df.loc[(df['emoji'].str.len() != 0),:].shape[0]/df.shape[0])}%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emoji'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c69a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_emoji = df[df['emoji'].str.len() != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d091ab16",
   "metadata": {},
   "source": [
    "Cela n'est pas suffisant pour une prise en compte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5229d3b9",
   "metadata": {},
   "source": [
    "### 2.3.2. Preprocessing pour une analyse de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd16861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste de tous les emojis presents\n",
    "#res_list = [y for x in df['emoji'] for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89487ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste d'emojis uniques\n",
    "#emo_list = list(set(res_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa17a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(emo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7edfc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emoji = df[df['emoji'].str.len() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation de nouvelles colonnes correspondant aux sentiments exprimÃ©s\n",
    "\n",
    "df_emoji['positive_emoji'] = 0\n",
    "df_emoji['neutral_emoji'] = 0\n",
    "df_emoji['negative_emoji'] = 0\n",
    "\n",
    "positive_emoji = ['â¤ï¸','â¤','ğŸ˜','â™¥ï¸','ğŸ˜Š','ğŸ’•','ğŸ‘','ğŸ˜‚','ğŸ™Œ','ğŸ¤‘','ğŸ’–','âœ¨','ğŸ˜Š','ğŸ‰','ğŸ’','ğŸ˜','ğŸ˜ˆ','ğŸ˜ƒ','ğŸ˜','ğŸ˜','ğŸ˜˜','ğŸ’“','ğŸ˜‰','ğŸ˜¬','ğŸ˜„','ğŸ˜€','ğŸ˜œ','ğŸ’—','ğŸ˜Œ','ğŸ˜†','ğŸ˜›','ğŸ˜»','ğŸ™‹','â£ï¸','ğŸ™‚','ğŸ˜‡','ğŸ’','ğŸ˜','ğŸ˜‹','ğŸ¤—','ğŸ™†','ğŸ¤“','ğŸ˜š','ğŸ˜™','ğŸ˜¸','ğŸ˜¼','ğŸ˜º','ğŸ˜½']\n",
    "neutral_emoji = ['ğŸ™','ğŸ’œ','ğŸ’™','ğŸ‘½','ğŸ’›','ğŸ’Ÿ','ğŸ’š','ğŸ˜…','ğŸ™ƒ','ğŸ’©','ğŸ˜³','ğŸ™„','ğŸ˜‘','ğŸ™‡','ğŸ™','ğŸ˜','ğŸ˜¶']\n",
    "negative_emoji = ['ğŸ’¥','ğŸ’˜','ğŸ˜­','ğŸ˜±','ğŸ‘','ğŸ˜«','ğŸ˜¨','ğŸ˜¢','ğŸ’€','ğŸ¤”','ğŸ‘»','ğŸ˜“','ğŸ’¦','ğŸ˜¤','ğŸ˜©','ğŸ˜´','ğŸ’”','ğŸ˜’','ğŸ˜ª','ğŸ˜ˆ','ğŸ˜£','ğŸ˜®','ğŸ˜¡','ğŸ˜•','ğŸ˜”','ğŸ˜ ','ğŸ˜·','ğŸ˜¥','ğŸ˜','ğŸ˜²','ğŸ˜°','ğŸ™€','ğŸ˜–','ğŸ˜§','ğŸ˜Ÿ','ğŸ˜¹','ğŸ˜µ','ğŸ˜¶','ğŸ˜¯','ğŸ¤’','ğŸ¤•','ğŸ˜¾','ğŸ’¤']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b86993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, text in enumerate(df_emoji['emoji']):\n",
    "    \n",
    "    for emoj in text:\n",
    "        \n",
    "        if emoj in positive_emoji:\n",
    "            df_emoji['positive_emoji'].iloc[idx] += 1\n",
    "        elif emoj in negative_emoji:\n",
    "            df_emoji['negative_emoji'].iloc[idx] += 1\n",
    "        else:\n",
    "            df_emoji['neutral_emoji'].iloc[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7c61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Voici le bilan de l'analyse de sentiments: \\n\")\n",
    "print(f\"Score positif: {df_emoji['positive_emoji'].value_counts()} \" )\n",
    "print(f\"Score nÃ©gatif: {df_emoji['negative_emoji'].value_counts()} \" )\n",
    "print(f\"Score neutre:  {df_emoji['neutral_emoji'].value_counts()} \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d0aec",
   "metadata": {},
   "source": [
    "## 2.4. Suppression des hastags, url, arobase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b31e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3f00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove urls, hashtags, arobase (et emoji)\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+',' ', text)\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    return re.sub(r'#\\S+',' ', text)\n",
    "\n",
    "def remove_arobase(text):\n",
    "    return re.sub(r'@\\S+',' ', text)\n",
    "\n",
    "def remove_emojis(text):\n",
    "    string = ' '.join([word for word in text if word not in emo_list])\n",
    "    return string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a1b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_clean'] = df['content'].apply(remove_hashtags)\n",
    "df['content_clean'] = df['content_clean'].apply(remove_urls)\n",
    "df['content_clean'] = df['content_clean'].apply(remove_arobase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df05b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['content','content_clean']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705c0efb",
   "metadata": {},
   "source": [
    "# 3. Listes de stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6a4131",
   "metadata": {},
   "source": [
    "## 3.1. Module ntk: french_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CrÃ©ation d'une liste de stopword en franÃ§ais Ã  partir de nltk\n",
    "french_stopwords_list = stopwords.words('french')\n",
    "\n",
    "# Suppression des accents\n",
    "french_stopwords_list=[unidecode(x) for x in french_stopwords_list]\n",
    "\n",
    "# ajout ukraine, guerre et annee\n",
    "french_stopwords_list.append('ukraine')\n",
    "french_stopwords_list.append('guerre')\n",
    "french_stopwords_list.append('annee')\n",
    "\n",
    "#print(f\"Ci-dessous, la liste des stopwords en franÃ§ais de nltk :\\n{french_stopwords_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f740f",
   "metadata": {},
   "source": [
    "## 3.2 Module stopwords: french_stopwords_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9ef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f57a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords_list_2 = get_stop_words('french')\n",
    "#print(f\"Ci-dessous, la liste des stopwords en franÃ§ais de stop_words :\\n{french_stopwords_list_2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd96c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des accents\n",
    "\n",
    "french_stopwords_list_2=[unidecode(x) for x in french_stopwords_list_2]\n",
    "#print(f\"Ci-dessous, la liste des stopwords SANS ACCENT de stop_words :\\n{french_stopwords_list_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b99d01",
   "metadata": {},
   "source": [
    "## 3.3 Fusion des 2 listes: french_stopwords_list_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb30581",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_stopwords_list_3 = set(french_stopwords_list + french_stopwords_list_2)\n",
    "french_stopwords_list_3 = sorted (french_stopwords_list_3 )\n",
    "\n",
    "print(f\"Ci-dessous, la liste complÃ¨te des stopwords en franÃ§ais :\\n{french_stopwords_list_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf75335",
   "metadata": {},
   "source": [
    "# 4. PrÃ©traitement (suite) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c40c81",
   "metadata": {},
   "source": [
    "## 4.1 PrÃ©traitement: Nettoyage complet = suppression des stopwords, etc..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d16517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation d'une fonction pour supprimer les stop words\n",
    "def no_stop_word(string, stopWords):\n",
    "\n",
    "    \"\"\"\n",
    "    Supprime les stop words d'un texte.\n",
    "\n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "\n",
    "    string : chaine de caractÃ¨re.\n",
    "\n",
    "    stopWords : liste de mots Ã  exclure. \n",
    "    \"\"\"\n",
    "    \n",
    "    string = ' '.join([word for word in string.split() if word not in stopWords])\n",
    "    return string\n",
    "\n",
    "# CrÃ©ation de la fonction de NETTOYAGE COMPLET\n",
    "def final_cleaner(pandasSeries, stopWords):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stemmatise une Series Pandas de documents \n",
    "\n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "    \n",
    "    pandasSeries : Une Series Pandas\n",
    "\n",
    "    stemmer : Stemmer de NLTK\n",
    "    \n",
    "    stopWords : Une liste de stopWords\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"#### Nettoyage en cours ####\") \n",
    "    \n",
    "    # confirmation que chaque article est bien de type str\n",
    "    pandasSeries = pandasSeries.apply(str)\n",
    "        \n",
    "    # Passage en minuscule\n",
    "    print(\"... Passage en minuscule\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x : x.lower())\n",
    "    \n",
    "    # Suppression des accents\n",
    "    print(\"... Suppression des accents\") \n",
    "    pandasSeries = pandasSeries.apply(unidecode)\n",
    "    \n",
    "    # DÃ©tection du champs annÃ©e\n",
    "    print(\"... DÃ©tection du champs annÃ©e\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x : re.sub(r'[0-9]{4}', 'annee', x))\n",
    "    \n",
    "    # Suppression http\n",
    "    #print(\"... Suppression http\") \n",
    "    #pandasSeries = pandasSeries.apply(lambda x : re.sub(r'https://', ' ', x))\n",
    "    \n",
    "    # Suppression des caractÃ¨res spÃ©ciaux et numÃ©riques\n",
    "    print(\"... Suppression des caractÃ¨res spÃ©ciaux et numÃ©riques\") \n",
    "    pandasSeries = pandasSeries.apply(lambda x :re.sub(r\"[^a-z]+\", ' ', x))\n",
    "    \n",
    "    # Suppression des stop words\n",
    "    print(\"... Suppression des stop words\") \n",
    "    #stopWords = [unidecode(sw) for sw in stopWords]\n",
    "    pandasSeries = pandasSeries.apply(lambda x : no_stop_word(x, stopWords))\n",
    " \n",
    "    print(\"#### Nettoyage OK! ####\")\n",
    "\n",
    "    return pandasSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15658f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du nettoyage final\n",
    "df['content_clean_final'] = final_cleaner(df['content_clean'], french_stopwords_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba31253",
   "metadata": {},
   "source": [
    "## 4.2 STEMMATISATION et LEMMATISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aff813",
   "metadata": {},
   "source": [
    "### 4.2.1 STEMMATISATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d27a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation d'une fonction de STEMMATISATION\n",
    "\n",
    "def stemmatise_text(text,stemmer):\n",
    "\n",
    "    \"\"\"\n",
    "    Stemmatise un texte : RamÃ¨ne les mots d'un texte Ã  leur racine (peut crÃ©er des mots qui n'existe pas).\n",
    "\n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "\n",
    "    text : Chaine de caractÃ¨res.\n",
    "\n",
    "    stemmer : Stemmer de NLTK.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93da3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise un stemmer NLTK\n",
    "stemmer = SnowballStemmer('french')\n",
    "\n",
    "\n",
    "# Application de la fonction stemmatise_text\n",
    "df['content_stem'] = df['content_clean_final'].apply(lambda x : stemmatise_text(x,stemmer))\n",
    "\n",
    "df[['content_clean_final', 'content_stem']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17df0a3",
   "metadata": {},
   "source": [
    "### 4.2.2 LEMMATISATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a1f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation d'une fonction de LEMMATISATION\n",
    "\n",
    "def lemmatise_text(text):\n",
    "\n",
    "    \"\"\"\n",
    "   Lemmatise un texte \n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "\n",
    "    text : Chaine de caractÃ¨res.\n",
    "\n",
    "    lemmer : lemmer de simplema\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join([simplemma.lemmatize(word, lang='fr') for word in text.split()])\n",
    "\n",
    "# Application de la fonction lemmatise_text\n",
    "df['content_lem'] = df['content_clean_final'].apply(lambda x : lemmatise_text(x))\n",
    "\n",
    "df[['content_stem', 'content_lem']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53932e31",
   "metadata": {},
   "source": [
    "### 4.2.3 Filtrage des tweets qui contiennent trop peu de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341242b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille avant filtrage\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6982c8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrage \n",
    "df = df[df['content_lem'].str.len() >= 3]\n",
    "# Taille aprÃ¨s filtrage\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaaf55c",
   "metadata": {},
   "source": [
    "## 4.3. Visualisation des MOTS LES PLUS UTILISES aprÃ¨s traitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67fa3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©ation d'une variable contenant le nombre de \"mots\" de chaque article\n",
    "df['nb_words_lem'] = df['content_lem'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Affichage du dataframe df\n",
    "#df.head()\n",
    "\n",
    "# RÃ©partition des tweets en fonction du nombre de mots\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.hist(df['nb_words_lem'], bins=30, color='b', edgecolor='k')\n",
    "_ = ax.set_xlabel('Number of words after preprocessing')\n",
    "_ = ax.set_ylabel('Number of tweets')\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe01400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nb_words_lem'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['nb_words_lem'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f25672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['nb_words_lem']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64b3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "import wordcloud \n",
    "\n",
    "# Join the different processed tweets together.\n",
    "long_string = ' '.join(df['content_stem'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wc = wordcloud.WordCloud(width=400,\n",
    "                      height=330,\n",
    "                      max_words=50,\n",
    "                      colormap='tab20c',\n",
    "                      collocations=True)\n",
    "\n",
    "# Generate a word cloud\n",
    "wc.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.title('Words Clouds', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3394859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour afficher les mots les plus utilisÃ©s \n",
    "def print_words(df , col, nb_words):\n",
    "    \n",
    "    \"\"\"\n",
    "   print les max_words mots les plus frÃ©quemment utilisÃ©s par cluster\n",
    "\n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame Pandas\n",
    "    \n",
    "    col : La sÃ©rie de df Ã  analyser (aprÃ¨s prÃ©-processing)\n",
    "\n",
    "    nb_words : nombre de mots\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "        \n",
    "    data = df[col]\n",
    "        \n",
    "    long_string = ' '.join(data)\n",
    "        \n",
    "    my_counts =  Counter(re.findall('\\w+', long_string))\n",
    "  \n",
    "    most_occur = my_counts.most_common(nb_words)\n",
    "  \n",
    "    #print(f\"Top {nb_words} :\\n {most_occur}.\")\n",
    "    \n",
    "    return most_occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list=print_words(df , 'content_lem', 20)\n",
    "df_top = pd.DataFrame(top_list, columns= ['Word','Count'])\n",
    "df_top.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9f3ae0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "plt.bar(df_top['Word'], df_top['Count'])\n",
    "_ = ax.set_xticklabels(df_top['Word'], rotation=45)\n",
    "_ = ax.set_xlabel('Mot')\n",
    "_ = ax.set_ylabel('Nombre d\\'occurences')\n",
    "_=ax.tick_params(axis='both', which='major', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f85aa54",
   "metadata": {},
   "source": [
    "## 4.4. Analyse des hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67647c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de tweets pour lesquels il n'y a pas de hastag\n",
    "df['hashtags'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ca68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression de ces tweets qui sont peu nombreux\n",
    "df.dropna(subset=['hashtags'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c749b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui transforme les hashtags en une liste de mots\n",
    "def list_hashtags(list_txt):\n",
    "    if len(list_txt)==1:\n",
    "        return list_txt[0]\n",
    "    else: \n",
    "        return ' '.join(list_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d53d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_hashtags([df['hashtags'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hashtags_clean'] = df['hashtags'].apply(lambda x : list_hashtags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0683e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['hashtags', 'hashtags_clean']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516cca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the different processed tweets together.\n",
    "long_string_hashtags = ' '.join(df['hashtags_clean'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wc = wordcloud.WordCloud(width=400,\n",
    "                      height=330,\n",
    "                      max_words=50,\n",
    "                      colormap='tab20c',\n",
    "                      collocations=True)\n",
    "\n",
    "# Generate a word cloud\n",
    "wc.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.title('Words Clouds Hashtags', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a65a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list_hashtags=print_words(df , 'hashtags_clean', 20)\n",
    "df_top_hashtags = pd.DataFrame(top_list, columns= ['Word','Count'])\n",
    "df_top_hashtags.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca92e0",
   "metadata": {},
   "source": [
    "L'analyse des mots les plus utilisÃ©s dans les hashtags et les tweets donne Ã  peu prÃ¨s la mÃªme chose..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c25ad5",
   "metadata": {},
   "source": [
    "# 5. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfba7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des tweets qui contiennent moins de 3 mots aprÃ¨s lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a818e8",
   "metadata": {},
   "source": [
    "## 5.1 Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3309c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoriseur TFIDF: on ignore les mots prÃ©sents dans plus de 50% des tweets ou dans moins de 1000 tweets\n",
    "# On ne prend en compte que les bigrams et lres trigrams.\n",
    "tfidf = TfidfVectorizer(max_df=0.5,\n",
    "                        min_df=2000,\n",
    "                        ngram_range=(1,3))\n",
    "                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81afe30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectoriseur COUNT\n",
    "#count = CountVectorizer(min_df=1000,\n",
    "                        #max_df=0.5,\n",
    "                        #ngram_range=(1, 3), # sÃ©lection bigrammes\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf.fit_transform([x for x in df[\"content_lem\"]])\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb3109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_matrix = count.fit_transform([x for x in df[\"content_lem\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(count_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words_tfidf = tfidf.get_feature_names()\n",
    "#type(list_words_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9a8e0",
   "metadata": {},
   "source": [
    "https://kavita-ganesan.com/hashingvectorizer-vs-countvectorizer/#.YzVS_YRBwuW\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c681748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WordCloud object\n",
    "wc = wordcloud.WordCloud(width=400,\n",
    "                      height=330,\n",
    "                      max_words=50,\n",
    "                      colormap='tab20c',\n",
    "                      collocations=True)\n",
    "\n",
    "# Generate a word cloud\n",
    "wc.generate(' '.join(list_words_tfidf))\n",
    "\n",
    "# Visualize the word cloud\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.title('Words Clouds TFIDF', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e8d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances =[]\n",
    "K = range(1,30,2)\n",
    "for k in K:\n",
    "    km =KMeans(n_clusters =k)\n",
    "    km =km.fit(tfidf_matrix)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0dcb43",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f191236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de kmeans puis crÃ©ation d'une colonne cluster\n",
    "km = KMeans(\n",
    "        n_clusters=10,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=5)\n",
    "\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix or count_matrix\n",
    "km.fit_transform(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Create a column cluster to denote the generated cluster for each tweet\n",
    "df[\"cluster\"] = clusters\n",
    "\n",
    "# Display number of tweets per cluster \n",
    "df['cluster'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_words_clusters(df , col, col_clus,  nb_cluster, nb_words):\n",
    "    \n",
    "    \"\"\"\n",
    "   print les max_words mots les plus frÃ©quemment utilisÃ©s par cluster\n",
    "\n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame Pandas\n",
    "    \n",
    "    col : La sÃ©rie de df Ã  analyser (sur lesquelles les clusters ont Ã©tÃ© calculÃ©s)\n",
    "\n",
    "    nb_cluster : nombre de clusters Ã  prendre en compte\n",
    "    \n",
    "    nb_words : nombre de mots\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(nb_cluster):\n",
    "        \n",
    "        data = df[df[col_clus] == i][col]\n",
    "        \n",
    "        long_string = ' '.join(data)\n",
    "        \n",
    "        my_counts =  Counter(re.findall('\\w+', long_string))\n",
    "  \n",
    "        most_occur = my_counts.most_common(nb_words)\n",
    "  \n",
    "        print(f\"Top {nb_words} du cluster n = {i+1} :\\n {most_occur}.\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des diffÃ©rents types de tweets suivant les clusters:\n",
    "# Join the different processed titles together.\n",
    "\n",
    "def plot_words_clusters(df , col, col_clus,  nb_cluster, max_words):\n",
    "    \n",
    "    \"\"\"\n",
    "   Trace un words_clouds pour chaque cluster \n",
    "\n",
    "    ParamÃ¨tres\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame Pandas\n",
    "    \n",
    "    col : La sÃ©rie de df Ã  analyser (sur lesquelles les clusters ont Ã©tÃ© calculÃ©s)\n",
    "\n",
    "    nb_cluster : nombre de clusters Ã  prendre en compte\n",
    "    \n",
    "    max_words : nombre max de mots\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(nb_cluster):\n",
    "        \n",
    "        data = df[df[col_clus] == i][col]\n",
    "        \n",
    "        long_string = ' '.join(data)\n",
    "\n",
    "        # Create a WordCloud object\n",
    "        wc = wordcloud.WordCloud(width=400,\n",
    "                                height=330,\n",
    "                                max_words=max_words,\n",
    "                                colormap='tab20c',\n",
    "                                collocations=True)\n",
    "\n",
    "        # Generate a word cloud\n",
    "        wc.generate(long_string)\n",
    "\n",
    "        # Visualize the word cloud\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.imshow(wc)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Words Clouds pour le cluster n={i+1}', fontsize=13)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words_clusters(df,\"content_lem\", \"cluster\", 10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390a246",
   "metadata": {},
   "source": [
    "## 5.3 Kmeans aprÃ¨s rÃ©duction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6536a9a8",
   "metadata": {},
   "source": [
    "### 5.2.1. Kmeans avec rÃ©duction de dimension: avec TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RÃ©duction de dimension:\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=50), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(tfidf_matrix)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recherche du nombre de clusters optimum: mÃ©thode du coude\n",
    "\n",
    "Sum_of_squared_distances =[]\n",
    "K = range(1,40)\n",
    "for k in K:\n",
    "    km =KMeans(n_clusters =k)\n",
    "    km =km.fit(X_lsa)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n",
    "    \n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ea094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de kmeans aprÃ¨s rÃ©duction de dimension\n",
    "\n",
    "km = KMeans(\n",
    "        n_clusters=10,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=1)\n",
    "\n",
    "\n",
    "# Fit the k-means object with tfidf_matrix or count_matrix\n",
    "km.fit_transform(X_lsa)\n",
    "\n",
    "clusters = km.labels_.tolist()\n",
    "\n",
    "# Create a column cluster to denote the generated cluster for each tweet\n",
    "df[\"cluster_lsa\"] = clusters\n",
    "\n",
    "# Display number of tweets per cluster \n",
    "df['cluster_lsa'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bec5f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_words_clusters(df,\"content_lem\", \"cluster_lsa\", 10, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ce33b",
   "metadata": {},
   "source": [
    "# 6. Recherche de topics: LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998f47fc",
   "metadata": {},
   "source": [
    "## 6.1 LDA: First method (gensim+doc2bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c4bfc9",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://www.kaggle.com/code/vukglisovic/classification-combining-lda-and-word2vec\n",
    "\n",
    "https://towardsdatascience.com/lda-topic-modeling-with-tweets-deff37c0e131\n",
    "\n",
    "https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know\n",
    "\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse statistique\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533efffd",
   "metadata": {},
   "source": [
    "Certains tweets ne contiennent aucun mot aprÃ¨s traitement: on les supprime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f4b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_null = df[df['nb_words_lem']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [text.split() for text in df_non_null['content_lem']]\n",
    "all_words = [y for x in all_words for y in x]\n",
    "all_words_unique= list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc93320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e5bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Le corpus comporte {len(all_words_unique)} mots diffÃ©rents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ec449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = sorted(all_words_unique)\n",
    "#print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211ccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mots les plus frÃ©quemment utilisÃ©s\n",
    "word_freq = FreqDist(all_words)\n",
    "\n",
    "#word_freq.most_common(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594f50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve word and count from FreqDist tuples\n",
    "\n",
    "most_common_count = [x[1] for x in word_freq.most_common(30)]\n",
    "most_common_word = [x[0] for x in word_freq.most_common(30)]\n",
    "\n",
    "#create dictionary mapping of word count\n",
    "top_30_dictionary = dict(zip(most_common_word, most_common_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a879f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(colormap = 'Accent', background_color = 'black').generate_from_frequencies(top_30_dictionary)\n",
    "\n",
    "#plot with matplotlib\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig('top_30_cloud.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_text(text):\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64123ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_null['stem_tokens']=df_non_null['content_stem'].apply(lambda x : token_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tokens = df_non_null['stem_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f028dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcae034",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(documents=df_non_null['stem_tokens'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b6169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Le dictionnaire comporte {} mots.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_above=0.75, no_below=1000)\n",
    "\n",
    "dictionary.compactify()  # Reindexes the remaining words after filtering\n",
    "print(\"AprÃ¨s suppression des extrÃªmes, il reste {} mots.\".format(len(dictionary.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318748ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Voici les identifiants des mots de dictionnary aprÃ¨s suppression outliers:\\n\\n {dictionary.token2id}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b31fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "tweets_bow = [dictionary.doc2bow(tweet) for tweet in df_non_null['stem_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb8029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output will contain a vector for each tweet, in the form of (word id, frequency of word occurrence in document)\n",
    "# Les 3 premiers tweets:\n",
    "tweets_bow[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0619bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA: 5 topics\n",
    "\n",
    "k = 5\n",
    "tweets_lda = LdaModel(tweets_bow,\n",
    "                      num_topics = k,\n",
    "                      id2word = dictionary,\n",
    "                      random_state = 1,\n",
    "                      passes=10)\n",
    "\n",
    "tweets_lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd81a2d",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensimvis.prepare(tweets_lda, tweets_bow, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d336c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=tweets_lda, texts=df_non_null['stem_tokens'], dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e7b55a",
   "metadata": {},
   "source": [
    "## 6.2 LDA: Seconde mÃ©thode (sklearn avec count_matrix)\n",
    "https://github.com/bicachu/topic-modeling-health-tweets/blob/master/notebooks/LDA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb56c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define target number of topics\n",
    "n_topics = 10\n",
    "\n",
    "# Fit model\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, learning_method='online', batch_size=10000, \n",
    "                                          random_state=0, learning_decay=0.5, verbose=0)\n",
    "# Create topic matrix\n",
    "lda_topic_matrix = lda_model.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6347d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c654ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_keys = get_keys(lda_topic_matrix)\n",
    "lda_categories, lda_counts = keys_to_counts(lda_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function\n",
    "def get_top_n_words(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b399a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Display top 10 words for each topic\n",
    "top_n_words_lda = get_top_n_words(10, lda_keys, count_matrix, count) \n",
    "for i in range(len(top_n_words_lda)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lda[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #plt.xticks(rotation= )\n",
    "    #fig.autofmt_xdate(rotation= )\n",
    "    #ax.set_xticklabels(xlabels, rotation= )\n",
    "    #plt.setp(ax.get_xticklabels(), rotation=)\n",
    "    #ax.tick_params(axis='x', labelrotation= )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2465c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tweet counts by topics\n",
    "top_5_words = get_top_n_words(5, lda_keys, count_matrix, count) \n",
    "\n",
    "labels = ['Topic {}: \\n'.format(i) + top_5_words[i] for i in lda_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lda_categories, lda_counts);\n",
    "ax.set_xticks(lda_categories);\n",
    "ax.set_xticklabels(labels, rotation=45);\n",
    "ax.set_title('LDA topic counts');\n",
    "ax.set_ylabel('Number of tweets');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_topic_matrix_sample =  lda_topic_matrix_test.sample(n=10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df89fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_topic_matrix_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aca89e",
   "metadata": {},
   "source": [
    "# 7. Utilisation de Word2Vec + T SNE visualisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338e77fc",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/achintyatripathi/gensim-word2vec-usage-with-t-sne-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e41ebc",
   "metadata": {},
   "source": [
    "https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4#4-vocab-dict-became-key_to_index-for-looking-up-a-keys-integer-index-or-get_vecattr-and-set_vecattr-for-other-per-key-attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbbc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases,Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phrases() takes a list of list of words as input\n",
    "sent = [row.split() for row in df['content_lem']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(sent,min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6357d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da13cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90bd248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Voici la liste des mots et des bigrams avec le nombre d'occurences:\\n\\n {word_freq}.\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classement du mot le plus utilisÃ© au mot le moins utilisÃ©\n",
    "sorted(word_freq, key=word_freq.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8ab569",
   "metadata": {},
   "source": [
    "\n",
    "The parameters :\n",
    "\n",
    "    min_count = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "    window = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n",
    "\n",
    "    vector_size = int - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "    negative = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "    workers = int - Use these many worker threads to train the model (=faster training with multicore machines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fd26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97b61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buil the vocabulary\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dacdcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070a4681",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"victime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the array\n",
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89630ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3dc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsnescatterplot(model, word, list_names):\n",
    "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
    "    its list of most similar words, and a list of words.\n",
    "    \"\"\"\n",
    "    arrays = np.empty((0, 300), dtype='f')\n",
    "    word_labels = [word]\n",
    "    color_list  = ['red']\n",
    "\n",
    "    # adds the vector of the query word\n",
    "    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n",
    "    \n",
    "    # gets list of most similar words\n",
    "    close_words = model.wv.most_similar([word])\n",
    "    \n",
    "    # adds the vector for each of the closest words to the array\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n",
    "        word_labels.append(wrd_score[0])\n",
    "        color_list.append('blue')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "    \n",
    "    # adds the vector for each of the words from list_names to the array\n",
    "    for wrd in list_names:\n",
    "        wrd_vector = model.wv.__getitem__([wrd])\n",
    "        word_labels.append(wrd)\n",
    "        color_list.append('green')\n",
    "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
    "        \n",
    "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
    "    reduc = PCA(n_components=20).fit_transform(arrays)\n",
    "    \n",
    "    # Finds t-SNE coordinates for 2 dimensions\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "    \n",
    "    # Sets everything up to plot\n",
    "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
    "                       'y': [y for y in Y[:, 1]],\n",
    "                       'words': word_labels,\n",
    "                       'color': color_list})\n",
    "    \n",
    "    fig, _ = plt.subplots()\n",
    "    fig.set_size_inches(9, 9)\n",
    "    \n",
    "    # Basic plot\n",
    "    p1 = sns.regplot(data=df,\n",
    "                     x=\"x\",\n",
    "                     y=\"y\",\n",
    "                     fit_reg=False,\n",
    "                     marker=\"o\",\n",
    "                     scatter_kws={'s': 40,\n",
    "                                  'facecolors': df['color']\n",
    "                                 }\n",
    "                    )\n",
    "    \n",
    "    # Adds annotations one by one with a loop\n",
    "    for line in range(0, df.shape[0]):\n",
    "         p1.text(df[\"x\"][line],\n",
    "                 df['y'][line],\n",
    "                 '  ' + df[\"words\"][line].title(),\n",
    "                 horizontalalignment='left',\n",
    "                 verticalalignment='bottom', size='medium',\n",
    "                 color=df['color'][line],\n",
    "                 weight='normal'\n",
    "                ).set_size(15)\n",
    "\n",
    "    \n",
    "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
    "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
    "            \n",
    "    plt.title('t-SNE visualization for {}'.format(word.title()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad41d61",
   "metadata": {},
   "source": [
    " We will look at the relationships between a query word (in **red**), its most similar words in the model (in **blue**), and other words from the vocabulary (in **green**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsnescatterplot(w2v_model, 'victime',[t[0] for t in w2v_model.wv.most_similar(positive=[\"victime\"], topn=20)][10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e198add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_CLUSTERS=3\n",
    "\n",
    "\n",
    "X=w2v_model.wv.vectors\n",
    "\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "print (assigned_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
